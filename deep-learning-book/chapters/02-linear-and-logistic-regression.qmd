# Linear and Logistic Regression: The Foundation of Predictive Modeling {#sec-regression}

Imagine you're trying to predict tomorrow's temperature or determine whether an incoming email is spam. These everyday problems introduce us to two fundamental machine learning techniques: linear regression for continuous predictions (like temperature) and logistic regression for binary decisions (like spam detection). Let's explore how these methods work and become proficient in implementing them.

## The Art of Drawing Lines Through Data

### Linear Regression: Finding Patterns in Numbers

When we plot data points on a graph, our eyes naturally try to find patterns. Linear regression does this mathematically, finding the best line that describes the relationship between variables. The basic equation looks deceptively simple:

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon
$$

Here's what each piece means:
- $y$ is what we're trying to predict (like house prices)
- $\beta_0$ is where our line starts (the intercept)
- $\beta_1, \beta_2, ..., \beta_n$ are the slopes for each feature
- $x_1, x_2, ..., x_n$ are our input features
- $\epsilon$ represents the uncertainty in our predictions

Let's see this in action with both Python and R:

::: {.panel-tabset}

## Python
```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Create sample temperature data
np.random.seed(42)
weather_data = pd.DataFrame({
    'humidity': np.random.normal(60, 15, 100),
    'wind_speed': np.random.normal(10, 5, 100),
    'pressure': np.random.normal(1013, 5, 100)
})
# Temperature is influenced by these factors plus some noise
weather_data['temperature'] = (
    30 - 0.1 * weather_data['humidity'] 
    - 0.5 * weather_data['wind_speed']
    + 0.1 * (weather_data['pressure'] - 1013)
    + np.random.normal(0, 2, 100)
)

# Train model
X = weather_data[['humidity', 'wind_speed', 'pressure']]
y = weather_data['temperature']
model = LinearRegression().fit(X, y)

print(f"R² Score: {model.score(X, y):.3f}")
```

## R
```r
# Create sample temperature data
set.seed(42)
weather_data <- data.frame(
  humidity = rnorm(100, 60, 15),
  wind_speed = rnorm(100, 10, 5),
  pressure = rnorm(100, 1013, 5)
)

# Generate temperature
weather_data$temperature <- 
  30 - 0.1 * weather_data$humidity -
  0.5 * weather_data$wind_speed +
  0.1 * (weather_data$pressure - 1013) +
  rnorm(100, 0, 2)

# Fit model
model <- lm(temperature ~ humidity + wind_speed + pressure,
            data = weather_data)

# Print summary
summary(model)
```
:::

### Logistic Regression: Making Binary Decisions

While linear regression helps us predict continuous values, many real-world problems require yes/no decisions. Enter logistic regression, which uses the sigmoid function to transform linear predictions into probabilities:

$$
P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + ... + \beta_nx_n)}}
$$

This elegant equation takes any input and squeezes it between 0 and 1, perfect for classification tasks. Let's implement a simple spam detector:

::: {.panel-tabset}

## Python
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Create email dataset
np.random.seed(42)
emails = pd.DataFrame({
    'length': np.random.normal(100, 30, 500),
    'urls': np.random.poisson(2, 500),
    'suspicious_words': np.random.poisson(1.5, 500)
})

# Label emails as spam based on rules
emails['is_spam'] = (
    (emails['length'] > 150) & 
    (emails['urls'] > 3) | 
    (emails['suspicious_words'] > 2)
).astype(int)

# Train model
X = emails[['length', 'urls', 'suspicious_words']]
y = emails['is_spam']
X_train, X_test, y_train, y_test = train_test_split(X, y)
model = LogisticRegression().fit(X_train, y_train)

# Evaluate
print(classification_report(y_test, model.predict(X_test)))
```

## R
```r
# Create email dataset
set.seed(42)
emails <- data.frame(
  length = rnorm(500, 100, 30),
  urls = rpois(500, 2),
  suspicious_words = rpois(500, 1.5)
)

# Label spam
emails$is_spam <- as.integer(
  (emails$length > 150 & emails$urls > 3) |
  (emails$suspicious_words > 2)
)

# Fit model
spam_model <- glm(
  is_spam ~ length + urls + suspicious_words,
  family = binomial(link = "logit"),
  data = emails
)

# Print summary
summary(spam_model)
```
:::

## Understanding Model Performance

### Metrics That Matter

For linear regression:
- R² (R-squared): Measures how much variance our model explains
- RMSE (Root Mean Square Error): Average prediction error in original units
- MAE (Mean Absolute Error): Average absolute difference between predictions and actual values

For logistic regression:
- Accuracy: Overall correct predictions
- Precision: Accuracy of positive predictions
- Recall: Ability to find all positive cases
- F1-Score: Harmonic mean of precision and recall

Here's how to calculate these metrics:

::: {.panel-tabset}

## Python
```python
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

def evaluate_regression(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return {
        'RMSE': rmse,
        'R²': r2
    }

# Example usage
metrics = evaluate_regression(y_test, model.predict(X_test))
print(metrics)
```

## R
```r
evaluate_regression <- function(model) {
  predictions <- predict(model)
  rmse <- sqrt(mean((model$model$temperature - predictions)^2))
  r2 <- summary(model)$r.squared
  
  list(
    RMSE = rmse,
    R2 = r2
  )
}

# Example usage
metrics <- evaluate_regression(model)
print(metrics)
```
:::

## Best Practices and Common Pitfalls

1. **Data Preparation**
   - Always scale your features when they're on different scales
   - Check for and handle missing values
   - Look for and address outliers
   - Split your data into training and testing sets

2. **Model Assumptions**
   - Linear regression assumes:
     * Linear relationships between variables
     * Independent observations
     * Constant variance of errors
     * Normally distributed residuals
   - Logistic regression assumes:
     * Independent observations
     * No perfect multicollinearity
     * Linear relationship between log odds and features

3. **Feature Engineering**
   - Create interaction terms when variables work together
   - Transform skewed variables (log, square root)
   - Encode categorical variables appropriately

## Advanced Topics: Beyond the Basics

### Regularization: Preventing Overfitting

Both linear and logistic regression can benefit from regularization:

- L1 (Lasso): Encourages sparse solutions
- L2 (Ridge): Prevents large coefficients
- Elastic Net: Combines L1 and L2

::: {.panel-tabset}

## Python
```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge regression
ridge = Ridge(alpha=1.0).fit(X_train, y_train)

# Lasso regression
lasso = Lasso(alpha=1.0).fit(X_train, y_train)

# Elastic Net
elastic = ElasticNet(alpha=1.0, l1_ratio=0.5).fit(X_train, y_train)
```

## R
```r
# Ridge regression
library(glmnet)
ridge <- glmnet(
  as.matrix(X_train), y_train,
  alpha = 0  # Ridge
)

# Lasso regression
lasso <- glmnet(
  as.matrix(X_train), y_train,
  alpha = 1  # Lasso
)
```
:::

## Summary

Linear and logistic regression are fundamental building blocks in machine learning. Their simplicity, interpretability, and efficiency make them valuable tools for many real-world applications. As we move forward to more complex models like neural networks, remember that these concepts form the foundation of modern machine learning.

Try these exercises to reinforce your learning:

1. Predict house prices using multiple features
2. Build a simple credit card fraud detection system
3. Compare the performance of different regularization techniques

Remember: The best way to learn is by doing. Experiment with these examples using your own datasets!