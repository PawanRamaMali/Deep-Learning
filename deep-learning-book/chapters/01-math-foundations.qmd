
# Fundamentals of Deep Learning {#sec-fundamentals}

Deep learning is at the heart of many technological breakthroughs of our era, from self-driving cars and virtual assistants to advanced medical imaging and real-time language translation. Unlike traditional programming, where rules are explicitly coded, deep learning allows machines to learn from data, identifying patterns and relationships on their own. This ability stems from the power of neural networks, mathematical constructs inspired by the human brain.

In this chapter, we will establish the foundation for understanding deep learning by covering:
- The mathematical structures and operations it relies on.
- How neural networks are built and how they learn.
- The role of activation functions and optimization techniques in training these models.

---

## Introduction

Deep learning’s strength lies in its ability to model complex, high-dimensional relationships. Imagine a neural network tasked with classifying handwritten digits. Each pixel of an image contributes to the network's understanding, and the layers of the network collaborate to gradually build a representation, starting from simple edges to complete digit shapes. This layered learning approach is what distinguishes deep learning from conventional machine learning.

As engineers and technical practitioners, it’s important to understand not just the “what” but also the “how” behind deep learning. To achieve this, we first dive into the mathematical building blocks.

---

## Mathematical Foundations

### Scalars, Vectors, Matrices, and Tensors

The language of deep learning is linear algebra. At its core are four key data structures:

1. **Scalars**: A scalar is a single numerical value. For example, the temperature on a given day, \( x = 25 \), is a scalar. Scalars form the simplest data structure in deep learning and are often used as parameters, such as learning rates or biases.

2. **Vectors**: A vector is a one-dimensional array of numbers. Think of a student’s exam scores across three subjects:
   \[
   \mathbf{x} = [85, 90, 78]
   \]
   Here, each value corresponds to a feature of the data. Vectors are used to represent inputs, outputs, or weights in neural networks.

3. **Matrices**: A matrix is a two-dimensional array of numbers. Consider a table where rows correspond to students and columns to their scores in different subjects:
   \[
   A = \begin{bmatrix}
   85 & 90 & 78 \\
   88 & 92 & 80 \\
   79 & 85 & 88
   \end{bmatrix}
   \]
   Matrices are ubiquitous in deep learning, representing datasets, weight connections, or transformations.

4. **Tensors**: A tensor generalizes matrices to higher dimensions. For instance, a color image can be represented as a 3D tensor where width, height, and RGB color channels form its axes. Tensors allow deep learning models to handle data like video sequences or 3D medical scans.

---

### Key Operations in Linear Algebra

#### Matrix Transpose
The transpose of a matrix flips its rows and columns. For example:
\[
A^T = \begin{bmatrix}
85 & 88 & 79 \\
90 & 92 & 85 \\
78 & 80 & 88
\end{bmatrix}
\]
Transposing is often used in backpropagation, where gradients need to be propagated in reverse.

#### Dot Product
The dot product combines two vectors into a scalar. Consider:
\[
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^n a_i b_i
\]
If \( \mathbf{a} = [2, 3, 4] \) and \( \mathbf{b} = [5, 6, 7] \), the result is:
\[
\mathbf{a} \cdot \mathbf{b} = 2 \cdot 5 + 3 \cdot 6 + 4 \cdot 7 = 56
\]
This operation underpins the computation of weighted sums in neural networks.

#### Matrix Multiplication
Matrix multiplication combines two matrices \( A \) and \( B \), resulting in a third matrix \( C \). Each element of \( C \) is computed as:
\[
C_{ij} = \sum_k A_{ik} B_{kj}
\]
Matrix multiplication is the backbone of forward and backward passes in deep learning.

---

### Vector Spaces and Properties

A vector space provides a framework for representing and manipulating data. Key properties of a vector space include:
- **Closure under addition**: Adding any two vectors in the space produces another vector in the space.
- **Closure under scalar multiplication**: Multiplying a vector by a scalar produces another vector in the space.
- **Zero vector**: The space contains a vector with all elements as zero, acting as the additive identity.

For instance, the space of student grades (e.g., \( [85, 90] \) and \( [88, 92] \)) remains valid under these operations, enabling linear transformations.

---

## Neural Networks: The Building Blocks

### Perceptron

The perceptron is the simplest neural unit, introduced in the 1950s as a mathematical model of a biological neuron. It computes a weighted sum of inputs and applies an activation function:
\[
y = \sigma(\mathbf{w} \cdot \mathbf{x} + b)
\]

Where:
- \( \mathbf{w} \): Weights assigned to each input.
- \( \mathbf{x} \): Input vector.
- \( b \): Bias, shifting the activation function.
- \( \sigma \): Activation function, mapping the weighted sum to an output.

#### Example: College Admission
Imagine using a perceptron to decide college admissions:
- Inputs: GPA, test scores, and extracurricular activities.
- Weights: Importance of each factor.
- Output: Binary decision ("Admit" or "Reject").

### Feedforward Neural Networks

Feedforward networks generalize perceptrons by stacking multiple layers:
1. **Input Layer**: Represents the raw input data (e.g., pixel values).
2. **Hidden Layers**: Extract patterns and representations.
3. **Output Layer**: Maps these representations to final predictions.

Data flows through layers as:
\[
\mathbf{h}^{(l)} = \sigma(\mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
\]
Where \( \mathbf{W}^{(l)} \) are weights and \( \mathbf{b}^{(l)} \) are biases for layer \( l \).

---

## Activation Functions

Activation functions introduce non-linearity into networks, enabling them to model complex patterns.

1. **Sigmoid**
   \[
   \sigma(x) = \frac{1}{1 + e^{-x}}
   \]
   - Converts inputs to probabilities.
   - Commonly used in binary classification tasks.

2. **ReLU (Rectified Linear Unit)**
   \[
   \text{ReLU}(x) = \max(0, x)
   \]
   - Efficient for training deep networks.
   - Solves vanishing gradient issues but may suffer from "dead neurons."

3. **Softmax**
   \[
   \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
   \]
   - Normalizes outputs into probabilities for multi-class classification.

---

## Optimization Techniques

### Gradient Descent

The goal of training is to minimize a loss function \( \mathcal{L} \). Gradient descent achieves this by iteratively updating parameters:
\[
\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla \mathcal{L}
\]
Where:
- \( \eta \): Learning rate.
- \( \nabla \mathcal{L} \): Gradient of the loss with respect to parameters.

---

## Applications of Fundamentals

### Image Classification
Feedforward networks analyze image pixels to identify objects. For example, in handwriting recognition, the network distinguishes curves (edges) to identify digits.

### Regression
Regression predicts continuous values, such as house prices, using input features like square footage and location.

---

## Conclusion

This chapter introduced the essential mathematical and architectural foundations of deep learning. Equipped with these tools, you are now ready to explore advanced architectures and applications that demonstrate the true power of deep learning.
