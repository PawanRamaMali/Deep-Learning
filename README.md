# Deep-Learning

## 1. Neural Networks
- **Artificial Neural Networks (ANNs)**: Inspired by the human brain, these are the foundational models in deep learning.
- **Convolutional Neural Networks (CNNs)**: Specialized for image processing tasks.
- **Recurrent Neural Networks (RNNs)**: Designed for sequence data, such as time series or natural language.

## 2. Training Techniques
- **Backpropagation**: The algorithm used to train neural networks.
- **Gradient Descent**: An optimization algorithm to minimize the cost function.
- **Batch Normalization**: A technique to improve training speed and stability.
- **Dropout**: A regularization technique to prevent overfitting.

## 3. Advanced Architectures
- **Long Short-Term Memory (LSTM)**: A type of RNN that addresses the vanishing gradient problem.
- **Generative Adversarial Networks (GANs)**: Models that generate new data similar to the training data.
- **Autoencoders**: Used for unsupervised learning and dimensionality reduction.
- **Transformers**: Highly effective for natural language processing tasks.

## 4. Evaluation and Optimization
- **Loss Functions**: Functions like Mean Squared Error (MSE) or Cross-Entropy Loss used to measure the performance of a model.
- **Hyperparameter Optimization**: The process of tuning model parameters to improve performance.
- **Cross-Validation**: A technique to assess the generalizability of a model.

## 5. Applications
- **Natural Language Processing (NLP)**: Techniques for understanding and generating human language.
- **Computer Vision**: Techniques for interpreting and understanding visual information from the world.
- **Reinforcement Learning**: Training models to make sequences of decisions by rewarding desired behaviors.

## 6. Emerging Topics
- **Explainability and Interpretability**: Making models more understandable to humans.
- **Transfer Learning**: Reusing a pre-trained model on a new, but related task.
- **Causality**: Understanding cause-and-effect relationships in data.
